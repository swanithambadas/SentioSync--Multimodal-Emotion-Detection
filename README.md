<p align="center">
  <img src="assets/demo.gif" alt="SentioSync Demo" width="80%"/>
</p>

<h1 align="center">ğŸ’¡ SentioSync â€“ Multimodal Emotion Detection</h1>
<p align="center"><strong>Realtime emotion analysis with body pose & facial embeddings in Vision Transformers</strong></p>

<p align="center">
  <a href="#notebooks--scripts">ğŸ““ Notebooks & Scripts</a> â€¢ 
  <a href="#techniques">ğŸ”¬ Techniques</a> â€¢ 
  <a href="#technologies">ğŸ“š Technologies</a> â€¢ 
  <a href="#project-structure">ğŸ“‚ Project Structure</a> â€¢ 
  <a href="#contributors">ğŸ‘¥ Contributors</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue?logo=python&logoColor=white"/> 
  <img src="https://img.shields.io/badge/Streamlit-1.x-orange?logo=streamlit&logoColor=white"/> 
  <img src="https://img.shields.io/badge/PyTorch-1.10-red?logo=pytorch&logoColor=white"/> 
  <img src="https://img.shields.io/badge/Transformers-4.x-purple?logo=transformers&logoColor=white"/> 
  <img src="https://img.shields.io/badge/OpenCV-4.x-yellow?logo=opencv&logoColor=white"/>
</p>

---

## ğŸ““ Notebooks & Scripts

- **Streamlit App**: [`Code/app.py`](./Code/app.py) â€“ Image upload, model selector & inference loop  
- **Model Definitions**: [`Code/models.py`](./Code/models.py) â€“ Chunked, crossâ€attention & multiâ€stage ViT variants  
- **Quick Start**: [`Code/run.sh`](./Code/run.sh) â€“ Env setup & demo launch  
- **Experiments** (in `Code/`):  
  - `data_cleaning.ipynb` â€“ Dataset exploration & cleaning  
  - `emotic_data_extraction.ipynb` â€“ Emotic dataset processing  
  - `face_and_pose_embeddings.ipynb` â€“ CLIP & Keypointâ€RCNN feature extraction  
  - `cross_attention.ipynb`, `Chunked_VIT.ipynb`, `Simple_VIT.ipynb`, `complex_chunked_vit.ipynb`, `individual_vit.ipynb` â€“ Architecture variants  
  - `pytorch_models.ipynb` â€“ Prototyping pipelines  
- **Annotations**: `Code/annotations/Annotations.mat` â€“ Raw annotation data  
- **Pretrained Weights**: `Code/models/*.pt` â€“ Checkpoint files  

---

## ğŸ”¬ Techniques

- **Person Detection** with TorchVisionâ€™s [Faster R-CNN](https://pytorch.org/vision/stable/models.html#torchvision.models.detection.fasterrcnn_resnet50_fpn)  
- **Facial Embeddings** via OpenCV Haar cascades ([docs](https://docs.opencv.org/4.x/db/d28/tutorial_cascade_classifier.html)) + OpenAIâ€™s [CLIP](https://huggingface.co/openai/clip-vit-base-patch32)  
- **Pose Estimation** using [Keypoint-RCNN](https://pytorch.org/vision/stable/models.html#keypointrcnn_resnet50_fpn)  
- **Vision Transformers**  
  - Chunked & multiâ€stage pipelines for large inputs  
  - Crossâ€attention fusion of face & pose features  
- **Realtime UI** with [Streamlit](https://docs.streamlit.io/) for fast iteration  

---

## ğŸ“š Technologies

- **Python 3.8+**  
- **Streamlit** â€“ Interactive demos ([docs](https://docs.streamlit.io/))  
- **PyTorch & TorchVision** â€“ Deep learning & detection ([docs](https://pytorch.org/))  
- **Transformers (Hugging Face)** â€“ CLIP model ([docs](https://huggingface.co/docs/transformers/))  
- **OpenCV** â€“ Image I/O & Haar cascades ([docs](https://docs.opencv.org/4.x/))  
- **Pillow** â€“ Image handling ([docs](https://pillow.readthedocs.io/))  
- **NumPy & SciPy** â€“ Numerical ops ([NumPy](https://numpy.org/doc/), [SciPy](https://scipy.org/docs/))  

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ annotations/              # Raw Emotic .mat annotation files
â”‚   â”‚   â””â”€â”€ Annotations.mat
â”‚   â”œâ”€â”€ models/                   # Pretrained .pt checkpoints
â”‚   â”‚   â””â”€â”€ *.pt
â”‚   â”œâ”€â”€ app.py                    # Streamlit entry point
â”‚   â”œâ”€â”€ models.py                 # ViT & fusion architecture definitions
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ run.sh                    # Setup & launch script
â””â”€â”€ Report/
    â””â”€â”€ swanitha_manishbi_final_report.pdf  # Detailed write-up & results
