{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc417177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ba586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, json_path, split=\"train\"):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        self.samples = []\n",
    "        for item in data.get(split, []):\n",
    "            face = item.get(\"face_embedding\")\n",
    "            pose = item.get(\"pose_embedding\")\n",
    "            label = item.get(\"multi_hot\")\n",
    "            if face is None or pose is None or label is None:\n",
    "                continue\n",
    "            self.samples.append((face, pose, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        face, pose, label = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(face, dtype=torch.float32),\n",
    "            torch.tensor(pose, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.float32),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99a901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, threshold=0.5):\n",
    "    y_pred_bin = (y_pred > threshold).astype(int)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy_score(y_true, y_pred_bin),\n",
    "        \"precision\": precision_score(y_true, y_pred_bin, average='micro', zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred_bin, average='micro', zero_division=0),\n",
    "        \"f1\":        f1_score(y_true, y_pred_bin, average='micro', zero_division=0),\n",
    "    }\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels, all_preds = [], []\n",
    "    for face, pose, label in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        face, pose, label = face.to(device), pose.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(face, pose)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        all_labels.append(label.cpu().numpy())\n",
    "        all_preds.append(output.detach().cpu().numpy())\n",
    "    y_true = np.vstack(all_labels)\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    return total_loss / len(dataloader), compute_metrics(y_true, y_pred)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, mode=\"Validation\"):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for face, pose, label in tqdm(dataloader, desc=mode, leave=False):\n",
    "            face, pose, label = face.to(device), pose.to(device), label.to(device)\n",
    "            output = model(face, pose)\n",
    "            total_loss += criterion(output, label).item()\n",
    "            all_labels.append(label.cpu().numpy())\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "    y_true = np.vstack(all_labels)\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    return total_loss / len(dataloader), compute_metrics(y_true, y_pred)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "                epochs=20, save_path=\"best_vit.pt\"):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        train_loss, train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, criterion, device, mode=\"Validation\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Acc: {train_metrics['accuracy']:.4f} | \"\n",
    "              f\"Prec: {train_metrics['precision']:.4f} | \"\n",
    "              f\"Rec: {train_metrics['recall']:.4f} | \"\n",
    "              f\"F1: {train_metrics['f1']:.4f}\")\n",
    "        print(f\" Val  Loss: {val_loss:.4f} | \"\n",
    "              f\"Acc: {val_metrics['accuracy']:.4f} | \"\n",
    "              f\"Prec: {val_metrics['precision']:.4f} | \"\n",
    "              f\"Rec: {val_metrics['recall']:.4f} | \"\n",
    "              f\"F1: {val_metrics['f1']:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"â†’ Best model saved.\")\n",
    "\n",
    "def test_model(model, test_loader, criterion, device, save_path=\"best_vit.pt\"):\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for face, pose, label in test_loader:\n",
    "            face, pose, label = face.to(device), pose.to(device), label.to(device)\n",
    "            output = model(face, pose)\n",
    "            test_loss += criterion(output, label).item() * face.size(0)\n",
    "\n",
    "            preds = torch.argmax(output, dim=1).cpu().numpy()\n",
    "            truths = torch.argmax(label, dim=1).cpu().numpy()\n",
    "\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(truths)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    prec  = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec   = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1    = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | \"\n",
    "          f\"Acc: {acc:.4f} | \"\n",
    "          f\"Prec: {prec:.4f} | \"\n",
    "          f\"Rec: {rec:.4f} | \"\n",
    "          f\"F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd587c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkedMultiStageViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        face_dim: int = 512,\n",
    "        face_chunks: int = 8,\n",
    "        pose_dim: int = 34,\n",
    "        pose_chunks: int = 2,\n",
    "        hidden_dim: int = 256,\n",
    "        num_classes: int = 7,\n",
    "        n_heads: int = 4,\n",
    "        face_layers: int = 2,\n",
    "        pose_layers: int = 2,\n",
    "        fusion_layers: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert face_dim % face_chunks == 0, \"face_dim must divide evenly by face_chunks\"\n",
    "        assert pose_dim % pose_chunks == 0, \"pose_dim must divide evenly by pose_chunks\"\n",
    "\n",
    "        self.face_chunks = face_chunks\n",
    "        self.pose_chunks = pose_chunks\n",
    "        self.f_chunk_size = face_dim // face_chunks\n",
    "        self.p_chunk_size = pose_dim // pose_chunks\n",
    "\n",
    "        self.face_proj = nn.Linear(self.f_chunk_size, hidden_dim)\n",
    "        self.pose_proj = nn.Linear(self.p_chunk_size, hidden_dim)\n",
    "\n",
    "        face_enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, batch_first=True\n",
    "        )\n",
    "        self.face_enc = nn.TransformerEncoder(face_enc_layer, num_layers=face_layers)\n",
    "\n",
    "        pose_enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, batch_first=True\n",
    "        )\n",
    "        self.pose_enc = nn.TransformerEncoder(pose_enc_layer, num_layers=pose_layers)\n",
    "\n",
    "        total_tokens = 1 + face_chunks + pose_chunks\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.fusion_pos = nn.Parameter(torch.randn(1, total_tokens, hidden_dim))\n",
    "\n",
    "        fusion_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, batch_first=True\n",
    "        )\n",
    "        self.fusion_enc = nn.TransformerEncoder(fusion_layer, num_layers=fusion_layers)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, face, pose):\n",
    "        B = face.size(0)\n",
    "\n",
    "        f = face.view(B, self.face_chunks, self.f_chunk_size)        \n",
    "        f = self.face_proj(f)                                        \n",
    "        f = self.face_enc(f)                                         \n",
    "\n",
    "        p = pose.view(B, self.pose_chunks, self.p_chunk_size)        \n",
    "        p = self.pose_proj(p)                                        \n",
    "        p = self.pose_enc(p)                                        \n",
    "\n",
    "        cls = self.cls_token.expand(B, -1, -1)                       \n",
    "        seq = torch.cat([cls, f, p], dim=1)                          \n",
    "        seq = seq + self.fusion_pos                                 \n",
    "\n",
    "        fused = self.fusion_enc(seq)                                 \n",
    "        cls_out = fused[:, 0]                                        \n",
    "        return self.mlp_head(cls_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d8984d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"final_annotations.json\"\n",
    "batch_size = 32\n",
    "epochs     = 20\n",
    "save_path  = \"best_vit_complex.pt\"\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_ds = EmotionDataset(json_path, split=\"train\")\n",
    "val_ds   = EmotionDataset(json_path, split=\"val\")\n",
    "test_ds  = EmotionDataset(json_path, split=\"test\")\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_ld   = DataLoader(val_ds,   batch_size=batch_size)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "model     = ChunkedMultiStageViT().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b82dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2790 | Acc: 0.5649 | Prec: 0.8094 | Rec: 0.6198 | F1: 0.7020\n",
      " Val  Loss: 0.2632 | Acc: 0.5638 | Prec: 0.8200 | Rec: 0.6131 | F1: 0.7016\n",
      "â†’ Best model saved.\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2622 | Acc: 0.5625 | Prec: 0.8095 | Rec: 0.6268 | F1: 0.7065\n",
      " Val  Loss: 0.2577 | Acc: 0.5656 | Prec: 0.8059 | Rec: 0.6385 | F1: 0.7125\n",
      "â†’ Best model saved.\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2574 | Acc: 0.5623 | Prec: 0.8101 | Rec: 0.6288 | F1: 0.7081\n",
      " Val  Loss: 0.2576 | Acc: 0.5624 | Prec: 0.7985 | Rec: 0.6460 | F1: 0.7142\n",
      "â†’ Best model saved.\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2536 | Acc: 0.5607 | Prec: 0.8076 | Rec: 0.6334 | F1: 0.7099\n",
      " Val  Loss: 0.2565 | Acc: 0.5636 | Prec: 0.8245 | Rec: 0.6112 | F1: 0.7020\n",
      "â†’ Best model saved.\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2496 | Acc: 0.5619 | Prec: 0.8075 | Rec: 0.6386 | F1: 0.7132\n",
      " Val  Loss: 0.2568 | Acc: 0.5624 | Prec: 0.7943 | Rec: 0.6477 | F1: 0.7136\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2454 | Acc: 0.5649 | Prec: 0.8079 | Rec: 0.6458 | F1: 0.7178\n",
      " Val  Loss: 0.2573 | Acc: 0.5567 | Prec: 0.7899 | Rec: 0.6428 | F1: 0.7088\n",
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2401 | Acc: 0.5661 | Prec: 0.8087 | Rec: 0.6520 | F1: 0.7220\n",
      " Val  Loss: 0.2579 | Acc: 0.5558 | Prec: 0.7853 | Rec: 0.6523 | F1: 0.7126\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2342 | Acc: 0.5641 | Prec: 0.8061 | Rec: 0.6597 | F1: 0.7256\n",
      " Val  Loss: 0.2668 | Acc: 0.5498 | Prec: 0.7661 | Rec: 0.6465 | F1: 0.7012\n",
      "\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2267 | Acc: 0.5700 | Prec: 0.8101 | Rec: 0.6734 | F1: 0.7354\n",
      " Val  Loss: 0.2691 | Acc: 0.5438 | Prec: 0.7740 | Rec: 0.6460 | F1: 0.7043\n",
      "\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2194 | Acc: 0.5739 | Prec: 0.8133 | Rec: 0.6865 | F1: 0.7446\n",
      " Val  Loss: 0.2697 | Acc: 0.5444 | Prec: 0.7773 | Rec: 0.6419 | F1: 0.7032\n",
      "\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2100 | Acc: 0.5825 | Prec: 0.8195 | Rec: 0.7009 | F1: 0.7556\n",
      " Val  Loss: 0.2907 | Acc: 0.5213 | Prec: 0.7486 | Rec: 0.6551 | F1: 0.6987\n",
      "\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2005 | Acc: 0.5948 | Prec: 0.8277 | Rec: 0.7174 | F1: 0.7686\n",
      " Val  Loss: 0.2887 | Acc: 0.5104 | Prec: 0.7489 | Rec: 0.6432 | F1: 0.6920\n",
      "\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1911 | Acc: 0.6056 | Prec: 0.8340 | Rec: 0.7329 | F1: 0.7802\n",
      " Val  Loss: 0.3187 | Acc: 0.5181 | Prec: 0.7532 | Rec: 0.6262 | F1: 0.6839\n",
      "\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1823 | Acc: 0.6185 | Prec: 0.8428 | Rec: 0.7474 | F1: 0.7922\n",
      " Val  Loss: 0.2959 | Acc: 0.5104 | Prec: 0.7432 | Rec: 0.6503 | F1: 0.6937\n",
      "\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1715 | Acc: 0.6357 | Prec: 0.8488 | Rec: 0.7674 | F1: 0.8061\n",
      " Val  Loss: 0.3135 | Acc: 0.4999 | Prec: 0.7388 | Rec: 0.6157 | F1: 0.6717\n",
      "\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1622 | Acc: 0.6544 | Prec: 0.8597 | Rec: 0.7834 | F1: 0.8198\n",
      " Val  Loss: 0.3362 | Acc: 0.4590 | Prec: 0.6907 | Rec: 0.6290 | F1: 0.6584\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1526 | Acc: 0.6699 | Prec: 0.8689 | Rec: 0.7984 | F1: 0.8322\n",
      " Val  Loss: 0.3483 | Acc: 0.4910 | Prec: 0.7148 | Rec: 0.6480 | F1: 0.6798\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1446 | Acc: 0.6846 | Prec: 0.8747 | Rec: 0.8072 | F1: 0.8396\n",
      " Val  Loss: 0.3672 | Acc: 0.4496 | Prec: 0.6843 | Rec: 0.6611 | F1: 0.6725\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1346 | Acc: 0.7046 | Prec: 0.8843 | Rec: 0.8258 | F1: 0.8541\n",
      " Val  Loss: 0.3820 | Acc: 0.4353 | Prec: 0.6638 | Rec: 0.6432 | F1: 0.6533\n",
      "\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1259 | Acc: 0.7230 | Prec: 0.8931 | Rec: 0.8376 | F1: 0.8645\n",
      " Val  Loss: 0.3871 | Acc: 0.4819 | Prec: 0.7124 | Rec: 0.6419 | F1: 0.6753\n",
      "\n",
      "Test Loss: 0.2500 | Acc: 0.8182 | Prec: 0.7178 | Rec: 0.8182 | F1: 0.7427\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_ld, val_ld, optimizer, criterion, device, epochs, save_path)\n",
    "test_model(model, test_ld, criterion, device, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffeaf25",
   "metadata": {},
   "source": [
    "# Fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ffd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkedMultiStageViT_fine(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        face_dim: int = 512,\n",
    "        face_chunks: int = 8,\n",
    "        pose_dim: int = 34,\n",
    "        pose_chunks: int = 2,\n",
    "        hidden_dim: int = 256,\n",
    "        num_classes: int = 7,\n",
    "        n_heads: int = 4,\n",
    "        face_layers: int = 2,\n",
    "        pose_layers: int = 2,\n",
    "        fusion_layers: int = 4,\n",
    "        dropout: float = 0.2,           \n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert face_dim % face_chunks == 0, \"face_dim must divide evenly by face_chunks\"\n",
    "        assert pose_dim % pose_chunks == 0, \"pose_dim must divide evenly by pose_chunks\"\n",
    "\n",
    "        self.face_chunks = face_chunks\n",
    "        self.pose_chunks = pose_chunks\n",
    "        self.f_chunk_size = face_dim // face_chunks\n",
    "        self.p_chunk_size = pose_dim // pose_chunks\n",
    "\n",
    "        # projections + per-modality encoders\n",
    "        self.face_proj = nn.Linear(self.f_chunk_size, hidden_dim)\n",
    "        self.pose_proj = nn.Linear(self.p_chunk_size, hidden_dim)\n",
    "        fe = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads, dropout=dropout, batch_first=True)\n",
    "        pe = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads, dropout=dropout, batch_first=True)\n",
    "        self.face_enc = nn.TransformerEncoder(fe, num_layers=face_layers)\n",
    "        self.pose_enc = nn.TransformerEncoder(pe, num_layers=pose_layers)\n",
    "\n",
    "        # fusion\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.fusion_pos = nn.Parameter(torch.randn(1, 1 + face_chunks + pose_chunks, hidden_dim))\n",
    "        fusion_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads, dropout=dropout, batch_first=True)\n",
    "        self.fusion_enc = nn.TransformerEncoder(fusion_layer, num_layers=fusion_layers)\n",
    "\n",
    "        # a bit more dropout before head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP head (already had dropout, but weâ€™ve harmonized rates)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, face, pose):\n",
    "        B = face.size(0)\n",
    "\n",
    "        # chunk + encode face\n",
    "        f = face.view(B, self.face_chunks, self.f_chunk_size)\n",
    "        f = self.face_proj(f)\n",
    "        f = self.dropout(f)\n",
    "        f = self.face_enc(f)\n",
    "\n",
    "        # chunk + encode pose\n",
    "        p = pose.view(B, self.pose_chunks, self.p_chunk_size)\n",
    "        p = self.pose_proj(p)\n",
    "        p = self.dropout(p)\n",
    "        p = self.pose_enc(p)\n",
    "\n",
    "        # prep cls token + fuse\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        seq = torch.cat([cls, f, p], dim=1) + self.fusion_pos\n",
    "        fused = self.fusion_enc(seq)\n",
    "\n",
    "        # take cls, regularize, then head\n",
    "        cls_out = fused[:, 0]\n",
    "        cls_out = self.dropout(cls_out)\n",
    "        return self.mlp_head(cls_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15536bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Stops training if validation loss doesnâ€™t improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience: int = 3, min_delta: float = 0.0, verbose: bool = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss: float):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6371542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    epochs: int = 20,\n",
    "    save_path: str = \"best_vit_chunked_fine_tuned.pt\",\n",
    "    patience: int = 3\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopper = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        train_loss, train_metrics = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        val_loss, val_metrics = evaluate(\n",
    "            model, val_loader, criterion, device, mode=\"Validation\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Acc: {train_metrics['accuracy']:.4f} | \"\n",
    "            f\"Prec: {train_metrics['precision']:.4f} | \"\n",
    "            f\"Rec: {train_metrics['recall']:.4f} | \"\n",
    "            f\"F1: {train_metrics['f1']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\" Val  Loss: {val_loss:.4f} | \"\n",
    "            f\"Acc: {val_metrics['accuracy']:.4f} | \"\n",
    "            f\"Prec: {val_metrics['precision']:.4f} | \"\n",
    "            f\"Rec: {val_metrics['recall']:.4f} | \"\n",
    "            f\"F1: {val_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        # save best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"â†’ Best model saved.\")\n",
    "\n",
    "        # check early stopping\n",
    "        early_stopper(val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"!! Early stopping triggered -- exiting training loop.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f97b1c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2222 | Acc: 0.5747 | Prec: 0.8137 | Rec: 0.6786 | F1: 0.7400\n",
      " Val  Loss: 0.2706 | Acc: 0.5501 | Prec: 0.7774 | Rec: 0.6594 | F1: 0.7135\n",
      "â†’ Best model saved.\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2137 | Acc: 0.5817 | Prec: 0.8196 | Rec: 0.6941 | F1: 0.7517\n",
      " Val  Loss: 0.2779 | Acc: 0.5219 | Prec: 0.7488 | Rec: 0.6346 | F1: 0.6870\n",
      "EarlyStopping counter: 1 out of 4\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2062 | Acc: 0.5900 | Prec: 0.8252 | Rec: 0.7042 | F1: 0.7599\n",
      " Val  Loss: 0.2923 | Acc: 0.5021 | Prec: 0.7320 | Rec: 0.6351 | F1: 0.6801\n",
      "EarlyStopping counter: 2 out of 4\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1977 | Acc: 0.5989 | Prec: 0.8313 | Rec: 0.7161 | F1: 0.7694\n",
      " Val  Loss: 0.2896 | Acc: 0.5281 | Prec: 0.7623 | Rec: 0.6254 | F1: 0.6871\n",
      "EarlyStopping counter: 3 out of 4\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1884 | Acc: 0.6095 | Prec: 0.8376 | Rec: 0.7336 | F1: 0.7822\n",
      " Val  Loss: 0.2885 | Acc: 0.5184 | Prec: 0.7398 | Rec: 0.6617 | F1: 0.6986\n",
      "EarlyStopping counter: 4 out of 4\n",
      "!! Early stopping triggered -- exiting training loop.\n",
      "\n",
      "Test Loss: 0.2603 | Acc: 0.8111 | Prec: 0.7212 | Rec: 0.8111 | F1: 0.7489\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_ld, val_ld, optimizer, criterion, device,\n",
    "            epochs=20, save_path=\"best_vit_chunked_fine_tuned.pt\", patience=4)\n",
    "test_model(model, test_ld, criterion, device, save_path=\"best_vit_chunked_fine_tuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb437be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
